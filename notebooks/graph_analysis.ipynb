{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48aa345",
   "metadata": {},
   "source": [
    "# Quick Graph Dataset Analysis\n",
    "This notebook scans the graph files under `submodules/graph-token/graphs`, computes basic statistics (nodes, edges, degree stats, connected components), and shows simple visualizations for a quick sanity-check. It's intentionally conservative about how many files it reads so it stays fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff989df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a51977ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found datasets: ['ba', 'complete', 'er', 'path', 'sbm', 'sfn', 'star']\n"
     ]
    }
   ],
   "source": [
    "# Configure the graphs directory (absolute path ensures notebook runs from any cwd)\n",
    "BASE = Path('/data/young/capstone/graph-learning-benchmarks/submodules/graph-token/graphs')\n",
    "assert BASE.exists(), f'Graphs base dir not found: {BASE}'\n",
    "# list top-level dataset directories (e.g., 'ba', 'er', 'path', ...)\n",
    "datasets = sorted([p.name for p in BASE.iterdir() if p.is_dir()])\n",
    "print('Found datasets:', datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bfaa9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_graph(path: Path) -> dict:\n",
    "    \"\"\"Read a graphml file and return basic stats.\n",
    "    Returns a dict with: path, nodes, edges, avg_deg, max_deg, min_deg, deg_median, deg_std, components, largest_cc, avg_cc_size, directed, clustering\n",
    "    \"\"\"\n",
    "    try:\n",
    "        G = nx.read_graphml(path)\n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'path': str(path)}\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    directed = nx.is_directed(G)\n",
    "    if n > 0:\n",
    "        degrees = [d for _, d in G.degree()]\n",
    "        avg_deg = float(np.mean(degrees))\n",
    "        max_deg = int(np.max(degrees))\n",
    "        min_deg = int(np.min(degrees))\n",
    "        deg_median = float(np.median(degrees))\n",
    "        deg_std = float(np.std(degrees, ddof=0))\n",
    "    else:\n",
    "        degrees = []\n",
    "        avg_deg = max_deg = min_deg = deg_median = deg_std = 0\n",
    "    # connected components: use weakly connected for directed graphs and compute sizes\n",
    "    try:\n",
    "        if directed:\n",
    "            comp_iter = nx.weakly_connected_components(G)\n",
    "        else:\n",
    "            comp_iter = nx.connected_components(G)\n",
    "        comp_sizes = [len(c) for c in comp_iter]\n",
    "        comps = len(comp_sizes)\n",
    "        largest_cc = int(max(comp_sizes)) if comp_sizes else 0\n",
    "        avg_cc_size = float(np.mean(comp_sizes)) if comp_sizes else 0.0\n",
    "    except Exception:\n",
    "        comps = None\n",
    "        largest_cc = None\n",
    "        avg_cc_size = None\n",
    "    # clustering (undirected) - may raise for very large graphs, handle safely\n",
    "    try:\n",
    "        if directed:\n",
    "            clu = nx.average_clustering(G.to_undirected())\n",
    "        else:\n",
    "            clu = nx.average_clustering(G)\n",
    "    except Exception:\n",
    "        clu = None\n",
    "    return dict(path=str(path), nodes=int(n), edges=int(m), avg_deg=avg_deg, max_deg=max_deg, min_deg=min_deg, deg_median=deg_median, deg_std=deg_std, components=comps, largest_cc=largest_cc, avg_cc_size=avg_cc_size, directed=directed, clustering=clu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75190d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 2100 graph files (up to 100 per split).\n"
     ]
    }
   ],
   "source": [
    "# Scan dataset folders and analyze a limited number of graphs for a quick summary\n",
    "results = []\n",
    "limit_per_split = 100  # max files to read per dataset split to keep analysis quick\n",
    "for ds in datasets:\n",
    "    ds_dir = BASE / ds\n",
    "    # expected splits like train/valid/test under each dataset\n",
    "    for split in sorted([p for p in ds_dir.iterdir() if p.is_dir()]):\n",
    "        files = sorted(list(split.glob('*.graphml')))\n",
    "        if not files:\n",
    "            continue\n",
    "        to_take = files[:limit_per_split]\n",
    "        for p in to_take:\n",
    "            res = analyze_graph(p)\n",
    "            # attach metadata\n",
    "            if 'error' not in res:\n",
    "                res['dataset'] = ds\n",
    "                res['split'] = split.name\n",
    "            else:\n",
    "                res['dataset'] = ds\n",
    "                res['split'] = split.name\n",
    "            results.append(res)\n",
    "print(f'Analyzed {len(results)} graph files (up to {limit_per_split} per split).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cda8d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>nodes_count</th>\n",
       "      <th>nodes_mean</th>\n",
       "      <th>nodes_median</th>\n",
       "      <th>nodes_min</th>\n",
       "      <th>nodes_max</th>\n",
       "      <th>edges_mean</th>\n",
       "      <th>edges_median</th>\n",
       "      <th>avg_deg_mean</th>\n",
       "      <th>avg_deg_median</th>\n",
       "      <th>deg_std_mean</th>\n",
       "      <th>density_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ba</td>\n",
       "      <td>300</td>\n",
       "      <td>12.080000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>29.376667</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.350560</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>2.432691</td>\n",
       "      <td>0.201240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complete</td>\n",
       "      <td>300</td>\n",
       "      <td>12.073333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>75.633333</td>\n",
       "      <td>66.0</td>\n",
       "      <td>11.073333</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>er</td>\n",
       "      <td>300</td>\n",
       "      <td>12.086667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>23.5</td>\n",
       "      <td>5.413207</td>\n",
       "      <td>4.461538</td>\n",
       "      <td>1.125368</td>\n",
       "      <td>0.243620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>path</td>\n",
       "      <td>300</td>\n",
       "      <td>12.073333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>11.073333</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.807667</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.381498</td>\n",
       "      <td>0.096167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sbm</td>\n",
       "      <td>300</td>\n",
       "      <td>11.926667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>36.736667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.375482</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.670846</td>\n",
       "      <td>0.245675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sfn</td>\n",
       "      <td>300</td>\n",
       "      <td>12.073333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>15.360000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.499552</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.290154</td>\n",
       "      <td>0.132610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>star</td>\n",
       "      <td>300</td>\n",
       "      <td>12.073333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>11.073333</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.807667</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>2.686480</td>\n",
       "      <td>0.096167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  nodes_count  nodes_mean  nodes_median  nodes_min  nodes_max  \\\n",
       "0        ba          300   12.080000          12.0          5         19   \n",
       "1  complete          300   12.073333          12.0          5         19   \n",
       "2        er          300   12.086667          12.0          5         19   \n",
       "3      path          300   12.073333          12.0          5         19   \n",
       "4       sbm          300   11.926667          12.0          5         19   \n",
       "5       sfn          300   12.073333          12.0          5         19   \n",
       "6      star          300   12.073333          12.0          5         19   \n",
       "\n",
       "   edges_mean  edges_median  avg_deg_mean  avg_deg_median  deg_std_mean  \\\n",
       "0   29.376667          22.0      4.350560        3.750000      2.432691   \n",
       "1   75.633333          66.0     11.073333       11.000000      0.000000   \n",
       "2   37.416667          23.5      5.413207        4.461538      1.125368   \n",
       "3   11.073333          11.0      1.807667        1.833333      0.381498   \n",
       "4   36.736667          30.0      5.375482        5.000000      1.670846   \n",
       "5   15.360000          15.0      2.499552        2.500000      2.290154   \n",
       "6   11.073333          11.0      1.807667        1.833333      2.686480   \n",
       "\n",
       "   density_mean  \n",
       "0      0.201240  \n",
       "1      0.500000  \n",
       "2      0.243620  \n",
       "3      0.096167  \n",
       "4      0.245675  \n",
       "5      0.132610  \n",
       "6      0.096167  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build DataFrame and show summary\n",
    "df = pd.DataFrame(results)\n",
    "# separate error rows if any\n",
    "if 'error' in df.columns:\n",
    "    error_df = df[df['error'].notnull()]\n",
    "else:\n",
    "    error_df = pd.DataFrame([])\n",
    "# Convert numeric columns\n",
    "num_cols = ['nodes','edges','avg_deg','max_deg','min_deg','components','clustering','deg_median','deg_std','largest_cc','avg_cc_size']\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "# Build a safe boolean mask for rows without errors\n",
    "if 'error' in df.columns:\n",
    "    good_mask = df['path'].notnull() & df['error'].isna()\n",
    "else:\n",
    "    good_mask = df['path'].notnull()\n",
    "# Derived metrics: density (undirected formula), edges per node\n",
    "df.loc[:, 'density'] = df.apply(lambda r: (r['edges'] / (r['nodes'] * (r['nodes'] - 1))) if pd.notnull(r['nodes']) and r['nodes'] > 1 else 0.0, axis=1)\n",
    "df.loc[:, 'edges_per_node'] = df.apply(lambda r: (r['edges'] / r['nodes']) if pd.notnull(r['nodes']) and r['nodes'] > 0 else 0.0, axis=1)\n",
    "# Summary by dataset (safe: use only good rows)\n",
    "summary = df[good_mask].groupby('dataset').agg({\n",
    "    'nodes':['count','mean','median','min','max'],\n",
    "    'edges':['mean','median'],\n",
    "    'avg_deg':['mean','median'],\n",
    "    'deg_std':['mean'],\n",
    "    'density':['mean'],\n",
    "})\n",
    "summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "summary = summary.reset_index()\n",
    "display(summary)\n",
    "if not error_df.empty:\n",
    "    print('Some files failed to read (showing up to 10):')\n",
    "    display(error_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0ed7fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No graph data available to plot.\n"
     ]
    }
   ],
   "source": [
    "# Simple plots using matplotlib (no seaborn)\n",
    "plot_df = df[df['path'].notnull() & df.get('error', pd.Series()).isna()]\n",
    "if plot_df.empty:\n",
    "    print('No graph data available to plot.')\n",
    "else:\n",
    "    # Boxplot of nodes by dataset\n",
    "    datasets = sorted(plot_df['dataset'].unique())\n",
    "    groups = [plot_df.loc[plot_df['dataset'] == d, 'nodes'].dropna().values for d in datasets]\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.boxplot(groups, labels=datasets, showfliers=False, patch_artist=True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Nodes')\n",
    "    plt.title('Nodes distribution by dataset (sample)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Scatter: edges vs nodes (log scales for readability)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    cmap = plt.get_cmap('tab20')\n",
    "    color_map = {d: cmap(i % 20) for i,d in enumerate(datasets)}\n",
    "    for d in datasets:\n",
    "        sub = plot_df[plot_df['dataset'] == d]\n",
    "        plt.scatter(sub['nodes'], sub['edges'], alpha=0.6, label=d, color=color_map[d], s=20)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Nodes (log)')\n",
    "    plt.ylabel('Edges (log)')\n",
    "    plt.title('Edges vs Nodes (sample, log-log)')\n",
    "    plt.legend(ncol=2, fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Histograms: avg_deg and max_deg\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(plot_df['avg_deg'].dropna(), bins=50, color='#4c72b0')\n",
    "    plt.title('Average degree (sample)')\n",
    "    plt.xlabel('avg_deg')\n",
    "    plt.subplot(1,2,2)\n",
    "    # log-binned histogram for max degree\n",
    "    max_deg = plot_df['max_deg'].dropna()\n",
    "    if not max_deg.empty:\n",
    "        bins = np.unique(np.logspace(0, math.log10(max(1, max_deg.max())), num=40).astype(int))\n",
    "        plt.hist(max_deg, bins=bins, color='#dd8452')\n",
    "        plt.xscale('log')\n",
    "    plt.title('Max degree (sample)')\n",
    "    plt.xlabel('max_deg (log scale)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree CCDF plots for a few small sampled graphs (log-log)\n",
    "from pathlib import Path\n",
    "viz_out = Path('viz')\n",
    "viz_out.mkdir(exist_ok=True)\n",
    "good = df[good_mask].copy()\n",
    "if 'nodes' in good.columns:\n",
    "    candidates = good[good['nodes'] <= 200]\n",
    "else:\n",
    "    candidates = good\n",
    "n_take = min(5, len(candidates))\n",
    "if n_take == 0:\n",
    "    print('No small graphs available for CCDF sampling. Increase threshold or ensure nodes were computed.')\n",
    "else:\n",
    "    sample_small = candidates.sample(n=n_take, random_state=42) if len(candidates) > n_take else candidates\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for _, row in sample_small.iterrows():\n",
    "        p = Path(row['path'])\n",
    "        try:\n",
    "            G = nx.read_graphml(p)\n",
    "        except Exception as e:\n",
    "            print('Failed to read', p, e)\n",
    "            continue\n",
    "        degs = np.array([d for _, d in G.degree()])\n",
    "        if degs.size == 0:\n",
    "            continue\n",
    "        vals, edges = np.histogram(degs, bins=range(0, int(degs.max())+2))\n",
    "        cdf = np.cumsum(vals) / vals.sum()\n",
    "        ccdf = 1.0 - cdf\n",
    "        plt.step(edges[:-1]+0.0, ccdf, where='post', label=f\"{row.get('dataset','?')}/{row.get('split','')}/{p.stem} (n={row.get('nodes','?')})\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Degree (log)')\n",
    "    plt.ylabel('CCDF P(Degree > k)')\n",
    "    plt.title('Degree CCDF for sampled small graphs')\n",
    "    plt.legend(fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    out = viz_out / 'degree_ccdf_sample.png'\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.show()\n",
    "    print('Saved', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analyses: aggregated degree PDFs, component fractions, metric correlations, centralities, path metrics\n",
    "from pathlib import Path\n",
    "viz_out = Path('viz')\n",
    "viz_out.mkdir(exist_ok=True)\n",
    "good = df[good_mask].copy()\n",
    "if good.empty:\n",
    "    print('No good graphs to analyze.')\n",
    "else:\n",
    "    # Aggregated degree distributions per dataset (sample up to 50 graphs per dataset)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    datasets = sorted(good['dataset'].unique())\n",
    "    cmap = plt.get_cmap('tab20')\n",
    "    for i,d in enumerate(datasets):\n",
    "        sub = good[good['dataset'] == d]\n",
    "        deg_values = []\n",
    "        sample_files = sub['path'].tolist()[:50]\n",
    "        for p in sample_files:\n",
    "            try:\n",
    "                G = nx.read_graphml(p)\n",
    "                deg_values.extend([dd for _, dd in G.degree()])\n",
    "            except Exception:\n",
    "                continue\n",
    "        if not deg_values:\n",
    "            continue\n",
    "        degs = np.array(deg_values)\n",
    "        maxk = max(1, int(degs.max()))\n",
    "        bins = np.logspace(0, math.log10(maxk + 1), num=50)\n",
    "        hist, edges = np.histogram(degs, bins=bins)\n",
    "        pdf = hist / hist.sum() if hist.sum() > 0 else hist\n",
    "        centers = (edges[:-1] + edges[1:]) / 2.0\n",
    "        plt.step(centers, pdf + 1e-12, where='mid', label=d, color=cmap(i % 20))\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Degree (log)')\n",
    "    plt.ylabel('PDF')\n",
    "    plt.title('Aggregated degree distribution by dataset (sampled graphs)')\n",
    "    plt.legend(fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    out = viz_out / 'degree_dist_by_dataset.png'\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.show()\n",
    "    print('Saved', out)\n",
    "\n",
    "    # Component size distribution: largest_cc fraction per dataset\n",
    "    plt.figure(figsize=(10,6))\n",
    "    groups = [ (good.loc[good['dataset']==d,'largest_cc'] / good.loc[good['dataset']==d,'nodes']).dropna().values for d in datasets ]\n",
    "    # filter empty groups\n",
    "    groups_filtered = [g for g in groups if len(g) > 0]\n",
    "    labels = [d for d,g in zip(datasets,groups) if len(g) > 0]\n",
    "    if groups_filtered:\n",
    "        plt.boxplot(groups_filtered, labels=labels, showfliers=False, patch_artist=True)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Largest CC fraction')\n",
    "        plt.title('Largest connected component fraction by dataset')\n",
    "        plt.tight_layout()\n",
    "        out = viz_out / 'largest_cc_fraction.png'\n",
    "        plt.savefig(out, dpi=150)\n",
    "        plt.show()\n",
    "        print('Saved', out)\n",
    "    else:\n",
    "        print('No component size data available.')\n",
    "\n",
    "    # Correlation matrix of numeric metrics\n",
    "    num_cols = ['nodes','edges','avg_deg','deg_median','deg_std','density','edges_per_node','largest_cc','avg_cc_size']\n",
    "    num = good[num_cols].dropna() if all(c in good.columns for c in num_cols) else good.select_dtypes(include=[np.number]).dropna()\n",
    "    if not num.empty:\n",
    "        corr = num.corr()\n",
    "        plt.figure(figsize=(8,6))\n",
    "        im = plt.imshow(corr, cmap='RdBu', vmin=-1, vmax=1)\n",
    "        plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "        ticks = range(len(corr.columns))\n",
    "        plt.xticks(ticks, corr.columns, rotation=45, ha='right')\n",
    "        plt.yticks(ticks, corr.columns)\n",
    "        for (i,j), val in np.ndenumerate(corr.values):\n",
    "            plt.text(j, i, f'{val:.2f}', ha='center', va='center', color='black', fontsize=8)\n",
    "        plt.title('Correlation matrix of graph metrics (sample)')\n",
    "        plt.tight_layout()\n",
    "        out = viz_out / 'metric_correlation.png'\n",
    "        plt.savefig(out, dpi=150)\n",
    "        plt.show()\n",
    "        print('Saved', out)\n",
    "    else:\n",
    "        print('Not enough numeric data for correlations.')\n",
    "\n",
    "    # Centrality summaries for up to 3 small graphs\n",
    "    small = good[good['nodes'] <= 200].copy() if 'nodes' in good.columns else good.copy()\n",
    "    if small.empty:\n",
    "        print('No small graphs for centrality computation.')\n",
    "    else:\n",
    "        sample = small.sample(n=min(3, len(small)), random_state=42)\n",
    "        for _, row in sample.iterrows():\n",
    "            p = Path(row['path'])\n",
    "            try:\n",
    "                G = nx.read_graphml(p)\n",
    "            except Exception as e:\n",
    "                print('Failed to read', p, e); continue\n",
    "            # operate on largest connected component\n",
    "            try:\n",
    "                if nx.is_directed(G):\n",
    "                    comp_nodes = max(nx.weakly_connected_components(G), key=len)\n",
    "                else:\n",
    "                    comp_nodes = max(nx.connected_components(G), key=len)\n",
    "                Gc = G.subgraph(comp_nodes).copy()\n",
    "            except Exception:\n",
    "                Gc = G\n",
    "            deg_cent = nx.degree_centrality(Gc)\n",
    "            clos = nx.closeness_centrality(Gc)\n",
    "            try:\n",
    "                bet = nx.betweenness_centrality(Gc, k=min(50, Gc.number_of_nodes()), seed=42) if Gc.number_of_nodes() > 100 else nx.betweenness_centrality(Gc)\n",
    "            except Exception:\n",
    "                bet = {n: 0.0 for n in Gc.nodes()}\n",
    "            def topk(d, k=10):\n",
    "                items = sorted(d.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "                if not items:\n",
    "                    return ([], [])\n",
    "                keys, vals = zip(*items)\n",
    "                return keys, vals\n",
    "            names_deg, vals_deg = topk(deg_cent, 10)\n",
    "            names_clo, vals_clo = topk(clos, 10)\n",
    "            names_bet, vals_bet = topk(bet, 10)\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "            if vals_deg:\n",
    "                axes[0].barh(range(len(vals_deg)), list(vals_deg)[::-1], color='#4c72b0')\n",
    "                axes[0].set_yticks(range(len(vals_deg))); axes[0].set_yticklabels(list(names_deg)[::-1])\n",
    "            axes[0].set_title('Degree centrality (top10)')\n",
    "            if vals_clo:\n",
    "                axes[1].barh(range(len(vals_clo)), list(vals_clo)[::-1], color='#dd8452')\n",
    "                axes[1].set_yticks(range(len(vals_clo))); axes[1].set_yticklabels(list(names_clo)[::-1])\n",
    "            axes[1].set_title('Closeness centrality (top10)')\n",
    "            if vals_bet:\n",
    "                axes[2].barh(range(len(vals_bet)), list(vals_bet)[::-1], color='#55a868')\n",
    "                axes[2].set_yticks(range(len(vals_bet))); axes[2].set_yticklabels(list(names_bet)[::-1])\n",
    "            axes[2].set_title('Betweenness centrality (top10)')\n",
    "            plt.suptitle(f\"Centralities for {row.get('dataset','?')}/{row.get('split','')}/{p.stem} (n={row.get('nodes','?')})\")\n",
    "            plt.tight_layout(rect=[0,0,1,0.95])\n",
    "            out = viz_out / f\"centrality_{p.stem}.png\"\n",
    "            plt.savefig(out, dpi=150)\n",
    "            plt.show()\n",
    "            print('Saved', out)\n",
    "\n",
    "    # Average shortest path and diameter for sampled small graphs\n",
    "    paths = []\n",
    "    for _, row in small.iterrows():\n",
    "        p = Path(row['path'])\n",
    "        try:\n",
    "            G = nx.read_graphml(p)\n",
    "        except Exception:\n",
    "            continue\n",
    "        try:\n",
    "            if nx.is_directed(G):\n",
    "                comps = list(nx.weakly_connected_components(G))\n",
    "            else:\n",
    "                comps = list(nx.connected_components(G))\n",
    "            largest = max(comps, key=len)\n",
    "            H = G.subgraph(largest).copy()\n",
    "            if H.number_of_nodes() > 1:\n",
    "                asp = nx.average_shortest_path_length(H)\n",
    "                diam = nx.diameter(H) if H.number_of_nodes() <= 1000 else None\n",
    "            else:\n",
    "                asp = 0; diam = 0\n",
    "            paths.append({'path': str(p), 'nodes': G.number_of_nodes(), 'avg_shortest_path': asp, 'diameter': diam})\n",
    "        except Exception:\n",
    "            continue\n",
    "    if paths:\n",
    "        paths_df = pd.DataFrame(paths)\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.scatter(paths_df['nodes'], paths_df['avg_shortest_path'])\n",
    "        plt.xscale('log'); plt.xlabel('Nodes (log)')\n",
    "        plt.ylabel('Avg shortest path length')\n",
    "        plt.title('Avg shortest path vs nodes (small graphs)')\n",
    "        out = viz_out / 'avg_shortest_path.png'\n",
    "        plt.savefig(out, dpi=150)\n",
    "        plt.show()\n",
    "        print('Saved', out)\n",
    "    else:\n",
    "        print('No path metrics computed.')\n",
    "\n",
    "    # Save summary CSV\n",
    "    out_csv = viz_out / 'graph_summary.csv'\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print('Saved summary CSV to', out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758dd3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few small graphs using networkx (up to 2 per dataset).\n",
    "from pathlib import Path\n",
    "viz_out = Path('viz')\n",
    "viz_out.mkdir(exist_ok=True)\n",
    "# use the safe good_mask computed earlier (falls back to path-only if no 'error' column)\n",
    "if 'error' in df.columns:\n",
    "    good_mask = df['path'].notnull() & df['error'].isna()\n",
    "else:\n",
    "    good_mask = df['path'].notnull()\n",
    "small = df[good_mask & (df['nodes'] <= 80)] if 'nodes' in df.columns else df[good_mask].head(10)\n",
    "if small.empty:\n",
    "    print('No small graphs found (nodes <= 80). Increase threshold or ensure nodes were computed.')\n",
    "else:\n",
    "    for ds, group in small.groupby('dataset'):\n",
    "        for _, row in group.head(2).iterrows():\n",
    "            p = Path(row['path'])\n",
    "            try:\n",
    "                G = nx.read_graphml(p)\n",
    "            except Exception as e:\n",
    "                print('Failed to read', p, e)\n",
    "                continue\n",
    "            # choose a layout - spring for general graphs\n",
    "            plt.figure(figsize=(6,6))\n",
    "            try:\n",
    "                pos = nx.spring_layout(G, seed=42)\n",
    "            except Exception:\n",
    "                pos = None\n",
    "            nx.draw(G, pos=pos, node_size=40, linewidths=0.1, edge_color='#999999', node_color='#1f78b4', with_labels=False)\n",
    "            title = f\"{ds}/{row.get('split','?')} - {p.name} (n={row.get('nodes','?')}, m={row.get('edges','?')})\"\n",
    "            plt.title(title)\n",
    "            out = viz_out / f\"{ds}_{row.get('split','')}_{p.stem}.png\"\n",
    "            plt.savefig(out, dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print('Saved', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c94737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community detection, assortativity, and spectral-gap analysis for sampled small graphs\n",
    "from pathlib import Path\n",
    "viz_out = Path('viz')\n",
    "viz_out.mkdir(exist_ok=True)\n",
    "good = df[good_mask].copy()\n",
    "small = good[good['nodes'] <= 400] if 'nodes' in good.columns else good\n",
    "if small.empty:\n",
    "    print('No small graphs for community/spectral analysis.')\n",
    "else:\n",
    "    sample = small.sample(n=min(20, len(small)), random_state=42) if len(small) > 20 else small\n",
    "    rows = []\n",
    "    for _, row in sample.iterrows():\n",
    "        p = Path(row['path'])\n",
    "        try:\n",
    "            G = nx.read_graphml(p)\n",
    "        except Exception as e:\n",
    "            print('Failed to read', p, e); continue\n",
    "        # work on largest (weak) connected component and undirected for community/spectral\n",
    "        try:\n",
    "            if nx.is_directed(G):\n",
    "                comps = list(nx.weakly_connected_components(G))\n",
    "            else:\n",
    "                comps = list(nx.connected_components(G))\n",
    "            largest = max(comps, key=len)\n",
    "            Gc = G.subgraph(largest).copy()\n",
    "            if nx.is_directed(Gc):\n",
    "                Gc = Gc.to_undirected()\n",
    "        except Exception:\n",
    "            Gc = G if not nx.is_directed(G) else G.to_undirected()\n",
    "        nodes = Gc.number_of_nodes()\n",
    "        num_comms = None; modularity = None\n",
    "        try:\n",
    "            comms = list(nx.algorithms.community.greedy_modularity_communities(Gc))\n",
    "            num_comms = len(comms)\n",
    "            modularity = nx.algorithms.community.quality.modularity(Gc, comms) if num_comms > 0 else None\n",
    "        except Exception:\n",
    "            num_comms = None; modularity = None\n",
    "        try:\n",
    "            assort = nx.degree_assortativity_coefficient(G)\n",
    "        except Exception:\n",
    "            assort = None\n",
    "        try:\n",
    "            trans = nx.transitivity(G)\n",
    "        except Exception:\n",
    "            trans = None\n",
    "        spectral_gap = None\n",
    "        if nodes > 1 and nodes <= 400:\n",
    "            try:\n",
    "                A = nx.to_numpy_array(Gc)\n",
    "                vals = np.linalg.eigvals(A)\n",
    "                vals = np.sort(np.real(vals))[::-1]\n",
    "                if len(vals) >= 2:\n",
    "                    spectral_gap = float(vals[0] - vals[1])\n",
    "                else:\n",
    "                    spectral_gap = float(vals[0])\n",
    "            except Exception:\n",
    "                spectral_gap = None\n",
    "        rows.append({'path': str(p), 'dataset': row.get('dataset'), 'nodes': int(row.get('nodes', nodes)), 'num_comms': num_comms, 'modularity': modularity, 'assortativity': assort, 'transitivity': trans, 'spectral_gap': spectral_gap})\n",
    "    cm_df = pd.DataFrame(rows)\n",
    "    if cm_df.empty:\n",
    "        print('No community/spectral metrics computed.')\n",
    "    else:\n",
    "        # Boxplot of modularity by dataset\n",
    "        plt.figure(figsize=(10,6))\n",
    "        datasets = sorted(cm_df['dataset'].dropna().unique())\n",
    "        groups = [cm_df.loc[cm_df['dataset'] == d, 'modularity'].dropna().values for d in datasets]\n",
    "        groups_f = [g for g in groups if len(g) > 0]\n",
    "        labels = [d for d,g in zip(datasets, groups) if len(g) > 0]\n",
    "        if groups_f:\n",
    "            plt.boxplot(groups_f, labels=labels, showfliers=False, patch_artist=True)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel('Modularity')\n",
    "            plt.title('Modularity (greedy) by dataset (sample)')\n",
    "            plt.tight_layout()\n",
    "            out = viz_out / 'community_modularity_by_dataset.png'\n",
    "            plt.savefig(out, dpi=150)\n",
    "            plt.show()\n",
    "            print('Saved', out)\n",
    "        else:\n",
    "            print('No modularity values to plot.')\n",
    "        # Scatter: assortativity vs avg_deg\n",
    "        if 'assortativity' in cm_df.columns and 'dataset' in cm_df.columns:\n",
    "            plt.figure(figsize=(8,6))\n",
    "            for i,d in enumerate(sorted(cm_df['dataset'].dropna().unique())):\n",
    "                sub = cm_df[cm_df['dataset'] == d]\n",
    "                plt.scatter(sub['nodes'], sub['assortativity'], label=d, alpha=0.7)\n",
    "            plt.xscale('log')\n",
    "            plt.xlabel('Nodes (log)')\n",
    "            plt.ylabel('Assortativity (degree)')\n",
    "            plt.title('Degree assortativity vs graph size (sample)')\n",
    "            plt.legend(fontsize='small')\n",
    "            plt.tight_layout()\n",
    "            out = viz_out / 'assortativity_vs_nodes.png'\n",
    "            plt.savefig(out, dpi=150)\n",
    "            plt.show()\n",
    "            print('Saved', out)\n",
    "        # Histogram of number of communities\n",
    "        if 'num_comms' in cm_df.columns and cm_df['num_comms'].notnull().any():\n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.hist(cm_df['num_comms'].dropna(), bins=range(1, int(cm_df['num_comms'].max())+2), color='#4c72b0')\n",
    "            plt.xlabel('Number of communities (greedy)')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Distribution of detected communities (sample)')\n",
    "            out = viz_out / 'num_communities_hist.png'\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(out, dpi=150)\n",
    "            plt.show()\n",
    "            print('Saved', out)\n",
    "        # Save community metrics CSV\n",
    "        out_cm = viz_out / 'community_metrics.csv'\n",
    "        cm_df.to_csv(out_cm, index=False)\n",
    "        print('Saved community metrics to', out_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d70e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7f8eec2",
   "metadata": {},
   "source": [
    "**Next steps / tips**\n",
    "- Run the notebook to produce the summary and plots.\n",
    "- To expand the analysis, increase `limit_per_split` or compute degree distributions per-graph.\n",
    "- If you want me to run this analysis now and save outputs (figures / CSV), tell me and I will execute the notebook and return results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autograph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
