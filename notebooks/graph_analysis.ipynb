{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48aa345",
   "metadata": {},
   "source": [
    "# Quick Graph Dataset Analysis\n",
    "This notebook scans the graph files under `submodules/graph-token/graphs`, computes basic statistics (nodes, edges, degree stats, connected components), and shows simple visualizations for a quick sanity-check. It's intentionally conservative about how many files it reads so it stays fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff989df8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      9\u001b[39m sns.set_style(\u001b[33m'\u001b[39m\u001b[33mwhitegrid\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mmatplotlib\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minline\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51977ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found datasets: ['ba', 'complete', 'er', 'path', 'sbm', 'sfn', 'star']\n"
     ]
    }
   ],
   "source": [
    "# Configure the graphs directory (absolute path ensures notebook runs from any cwd)\n",
    "BASE = Path('/data/young/capstone/graph-learning-benchmarks/submodules/graph-token/graphs')\n",
    "assert BASE.exists(), f'Graphs base dir not found: {BASE}'\n",
    "# list top-level dataset directories (e.g., 'ba', 'er', 'path', ...)\n",
    "datasets = sorted([p.name for p in BASE.iterdir() if p.is_dir()])\n",
    "print('Found datasets:', datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfaa9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_graph(path: Path) -> dict:\n",
    "    \"\"\"Read a graphml file and return basic stats.\n",
    "    Returns a dict with: dataset, split, filename, nodes, edges, avg_deg, max_deg, min_deg, components, directed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        G = nx.read_graphml(path)\n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'path': str(path)}\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    directed = nx.is_directed(G)\n",
    "    if n > 0:\n",
    "        degrees = [d for _, d in G.degree()]\n",
    "        avg_deg = sum(degrees) / n\n",
    "        max_deg = max(degrees)\n",
    "        min_deg = min(degrees)\n",
    "    else:\n",
    "        degrees = []\n",
    "        avg_deg = max_deg = min_deg = 0\n",
    "    # connected components: use weakly connected for directed graphs\n",
    "    try:\n",
    "        if directed:\n",
    "            comps = nx.number_weakly_connected_components(G)\n",
    "        else:\n",
    "            comps = nx.number_connected_components(G)\n",
    "    except Exception:\n",
    "        comps = None\n",
    "    # clustering (undirected) - may raise for very large graphs, handle safely\n",
    "    try:\n",
    "        if directed:\n",
    "            clu = nx.average_clustering(G.to_undirected())\n",
    "        else:\n",
    "            clu = nx.average_clustering(G)\n",
    "    except Exception:\n",
    "        clu = None\n",
    "    return dict(path=str(path), nodes=n, edges=m, avg_deg=avg_deg, max_deg=max_deg, min_deg=min_deg, components=comps, directed=directed, clustering=clu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75190d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 2100 graph files (up to 100 per split).\n"
     ]
    }
   ],
   "source": [
    "# Scan dataset folders and analyze a limited number of graphs for a quick summary\n",
    "results = []\n",
    "limit_per_split = 100  # max files to read per dataset split to keep analysis quick\n",
    "for ds in datasets:\n",
    "    ds_dir = BASE / ds\n",
    "    # expected splits like train/valid/test under each dataset\n",
    "    for split in sorted([p for p in ds_dir.iterdir() if p.is_dir()]):\n",
    "        files = sorted(list(split.glob('*.graphml')))\n",
    "        if not files:\n",
    "            continue\n",
    "        to_take = files[:limit_per_split]\n",
    "        for p in to_take:\n",
    "            res = analyze_graph(p)\n",
    "            # attach metadata\n",
    "            if 'error' not in res:\n",
    "                res['dataset'] = ds\n",
    "                res['split'] = split.name\n",
    "            else:\n",
    "                res['dataset'] = ds\n",
    "                res['split'] = split.name\n",
    "            results.append(res)\n",
    "print(f'Analyzed {len(results)} graph files (up to {limit_per_split} per split).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cda8d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>nodes_count</th>\n",
       "      <th>nodes_mean</th>\n",
       "      <th>nodes_median</th>\n",
       "      <th>nodes_min</th>\n",
       "      <th>nodes_max</th>\n",
       "      <th>edges_mean</th>\n",
       "      <th>edges_median</th>\n",
       "      <th>avg_deg_mean</th>\n",
       "      <th>avg_deg_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ba</td>\n",
       "      <td>300</td>\n",
       "      <td>12.080000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>29.376667</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.350560</td>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complete</td>\n",
       "      <td>300</td>\n",
       "      <td>12.073333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>75.633333</td>\n",
       "      <td>66.0</td>\n",
       "      <td>11.073333</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>er</td>\n",
       "      <td>300</td>\n",
       "      <td>12.086667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>23.5</td>\n",
       "      <td>5.413207</td>\n",
       "      <td>4.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>path</td>\n",
       "      <td>300</td>\n",
       "      <td>12.073333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>11.073333</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.807667</td>\n",
       "      <td>1.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sbm</td>\n",
       "      <td>300</td>\n",
       "      <td>11.926667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>36.736667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.375482</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sfn</td>\n",
       "      <td>300</td>\n",
       "      <td>12.073333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>15.360000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.499552</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>star</td>\n",
       "      <td>300</td>\n",
       "      <td>12.073333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>11.073333</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.807667</td>\n",
       "      <td>1.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  nodes_count  nodes_mean  nodes_median  nodes_min  nodes_max  \\\n",
       "0        ba          300   12.080000          12.0          5         19   \n",
       "1  complete          300   12.073333          12.0          5         19   \n",
       "2        er          300   12.086667          12.0          5         19   \n",
       "3      path          300   12.073333          12.0          5         19   \n",
       "4       sbm          300   11.926667          12.0          5         19   \n",
       "5       sfn          300   12.073333          12.0          5         19   \n",
       "6      star          300   12.073333          12.0          5         19   \n",
       "\n",
       "   edges_mean  edges_median  avg_deg_mean  avg_deg_median  \n",
       "0   29.376667          22.0      4.350560        3.750000  \n",
       "1   75.633333          66.0     11.073333       11.000000  \n",
       "2   37.416667          23.5      5.413207        4.461538  \n",
       "3   11.073333          11.0      1.807667        1.833333  \n",
       "4   36.736667          30.0      5.375482        5.000000  \n",
       "5   15.360000          15.0      2.499552        2.500000  \n",
       "6   11.073333          11.0      1.807667        1.833333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build DataFrame and show summary\n",
    "df = pd.DataFrame(results)\n",
    "# separate error rows if any\n",
    "if 'error' in df.columns:\n",
    "    error_df = df[df['error'].notnull()]\n",
    "else:\n",
    "    error_df = pd.DataFrame([])\n",
    "# Convert numeric columns\n",
    "for c in ['nodes','edges','avg_deg','max_deg','min_deg','components','clustering']:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "# Build a safe boolean mask for rows without errors\n",
    "if 'error' in df.columns:\n",
    "    good_mask = df['path'].notnull() & df['error'].isna()\n",
    "else:\n",
    "    good_mask = df['path'].notnull()\n",
    "summary = df[good_mask].groupby('dataset').agg({\n",
    "    'nodes':['count','mean','median','min','max'],\n",
    "    'edges':['mean','median'],\n",
    "    'avg_deg':['mean','median']\n",
    "})\n",
    "summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "summary = summary.reset_index()\n",
    "display(summary)\n",
    "if not error_df.empty:\n",
    "    print('Some files failed to read (showing up to 10):')\n",
    "    display(error_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ed7fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No graph data available to plot.\n"
     ]
    }
   ],
   "source": [
    "# Simple plots: nodes distribution across datasets (small sample).\n",
    "plot_df = df[df['path'].notnull() & df.get('error', pd.Series()).isna()]\n",
    "if not plot_df.empty:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.boxplot(x='dataset', y='nodes', data=plot_df)\n",
    "    plt.title('Nodes distribution by dataset (sample)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.scatterplot(x='nodes', y='edges', hue='dataset', data=plot_df, alpha=0.6)\n",
    "    plt.title('Edges vs Nodes (sample)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No graph data available to plot.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758dd3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few small graphs using networkx (up to 2 per dataset).\n",
    "from pathlib import Path\n",
    "viz_out = Path('notebooks/graph_viz_images')\n",
    "viz_out.mkdir(exist_ok=True)\n",
    "# use the safe good_mask computed earlier (falls back to path-only if no 'error' column)\n",
    "if 'error' in df.columns:\n",
    "    good_mask = df['path'].notnull() & df['error'].isna()\n",
    "else:\n",
    "    good_mask = df['path'].notnull()\n",
    "small = df[good_mask & (df['nodes'] <= 80)] if 'nodes' in df.columns else df[good_mask].head(10)\n",
    "if small.empty:\n",
    "    print('No small graphs found (nodes <= 80). Increase threshold or ensure nodes were computed.')\n",
    "else:\n",
    "    for ds, group in small.groupby('dataset'):\n",
    "        for _, row in group.head(2).iterrows():\n",
    "            p = Path(row['path'])\n",
    "            try:\n",
    "                G = nx.read_graphml(p)\n",
    "            except Exception as e:\n",
    "                print('Failed to read', p, e)\n",
    "                continue\n",
    "            # choose a layout - spring for general graphs\n",
    "            plt.figure(figsize=(6,6))\n",
    "            try:\n",
    "                pos = nx.spring_layout(G, seed=42)\n",
    "            except Exception:\n",
    "                pos = None\n",
    "            nx.draw(G, pos=pos, node_size=40, linewidths=0.1, edge_color='#999999', node_color='#1f78b4', with_labels=False)\n",
    "            title = f\"{ds}/{row.get('split','?')} - {p.name} (n={row.get('nodes','?')}, m={row.get('edges','?')})\"\n",
    "            plt.title(title)\n",
    "            out = viz_out / f\"{ds}_{row.get('split','')}_{p.stem}.png\"\n",
    "            plt.savefig(out, dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print('Saved', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8eec2",
   "metadata": {},
   "source": [
    "**Next steps / tips**\n",
    "- Run the notebook to produce the summary and plots.\n",
    "- To expand the analysis, increase `limit_per_split` or compute degree distributions per-graph.\n",
    "- If you want me to run this analysis now and save outputs (figures / CSV), tell me and I will execute the notebook and return results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autograph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
